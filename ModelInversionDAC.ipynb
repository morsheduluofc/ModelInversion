{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelInversionDAC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3fV1s_YWnGc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a44af2de-27fa-4a11-8e57-8f2b0a5f294e"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CISWqUwuWstp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2af89b56-b37a-4101-97ce-4dd075f934ce"
      },
      "source": [
        "#read all [194 users' oversampled data]\n",
        "import csv\n",
        "import pandas as pd\n",
        "with open('Data/AllOversampledNData.csv') as csvfile:\n",
        "    dataSet = list(csv.reader(csvfile, delimiter=','))\n",
        "dfdataSet = pd.DataFrame(dataSet[0:][:])\n",
        "dfdataSet.columns=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
        "            'F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','F31','F32','F33','F34','F35','F36','F37','F38','F39',\n",
        "            'F40','F41','F42','F43','F44','F45','F46','F47','F48','F49','F50','F51','F52','F53','F54','F55','F56','F57','F58','F59','F60',\n",
        "            'F61','F62','F63','F64','F65','ID']\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfnlSl3WyOX"
      },
      "source": [
        "#Include the data to [dataframe] and reset user id from [0 to 194]\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "columnsN=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
        "            'F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','F31','F32','F33','F34','F35','F36','F37','F38','F39',\n",
        "            'F40','F41','F42','F43','F44','F45','F46','F47','F48','F49','F50','F51','F52','F53','F54','F55','F56','F57','F58','F59','F60',\n",
        "            'F61','F62','F63','F64','F65','ID']\n",
        "columnsF=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
        "            'F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','F31','F32','F33','F34','F35','F36','F37','F38','F39',\n",
        "            'F40','F41','F42','F43','F44','F45','F46','F47','F48','F49','F50','F51','F52','F53','F54','F55','F56','F57','F58','F59','F60',\n",
        "            'F61','F62','F63','F64','F65']\n",
        "\n",
        "fdataSet = pd.DataFrame(columns = columnsN)\n",
        "\n",
        "#print(fdataSet)\n",
        "for i in range (0,195):\n",
        "  #fdataSet=fdataSet.append(shuffle(dfdataSet[1000*(i-1):i*1000]),ignore_index = True)\n",
        "  fdataSet=fdataSet.append(dfdataSet[1000*i:(i+1)*1000])\n",
        "\n",
        "fDataSet=fdataSet.drop(columns=['ID'])\n",
        "#fDataSet=standardize(fDataSet,columns=columnsF)\n",
        "\n",
        "fIDSet = pd.DataFrame(columns = ['ID'])\n",
        "fIDSet=fdataSet['ID']\n",
        "for i in range (0,195):\n",
        "  fIDSet[1000*i:(i+1)*1000]=i\n",
        "\n",
        "#fDataSet['ID'] = fIDSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEVPhppeW5AV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2368b262-7139-4a2f-ce44-39b7dc884b03"
      },
      "source": [
        "#Print [user ID] of user\n",
        "fDataSet=fDataSet[1:]\n",
        "fIDSet=fIDSet[1:]\n",
        "print(fIDSet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1           0\n",
            "2           0\n",
            "3           0\n",
            "4           0\n",
            "5           0\n",
            "         ... \n",
            "194995    194\n",
            "194996    194\n",
            "194997    194\n",
            "194998    194\n",
            "194999    194\n",
            "Name: ID, Length: 194999, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VuurVaWXJni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a85d5b15-d75b-495c-97c5-6c8c7eeb24a8"
      },
      "source": [
        "#Seperate the data in two groups and then divided them to tranning set, validation set and test set for 194 users\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X1=fDataSet[:96999]\n",
        "y1=fIDSet[:96999]\n",
        "X2=fDataSet[97000:193999]\n",
        "y2=fIDSet[97000:193999]\n",
        "\n",
        "y2=y2-97\n",
        "#print(y1.shape)\n",
        "#print(X2)\n",
        "#print(y2)\n",
        "\n",
        "#Divide first 97 uses data in tranning set, validation set and test set for 97 users\n",
        "#X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.30, random_state=22,stratify=y1)\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=22)\n",
        "X1_train, X1_val, y1_train, y1_val = train_test_split(X1_train, y1_train, test_size=0.2, random_state=22)\n",
        "\n",
        "y1_train = to_categorical(y1_train)\n",
        "y1_val = to_categorical(y1_val)\n",
        "y1_test = to_categorical(y1_test)\n",
        "\n",
        "#Divide next 97 uses data in tranning set, validation set and test set for 97 users\n",
        "#X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.30, random_state=22,stratify=y1)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=22)\n",
        "X2_train, X2_val, y2_train, y2_val = train_test_split(X2_train, y2_train, test_size=0.2, random_state=22)\n",
        "\n",
        "y2T=y2_train\n",
        "y2V=y2_val\n",
        "y2Te=y2_test\n",
        "\n",
        "y2_train = to_categorical(y2_train)\n",
        "y2_val = to_categorical(y2_val)\n",
        "y2_test = to_categorical(y2_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltI-QBP1X4gV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "af4264ef-2770-48cd-b2d6-8a399cfe7599"
      },
      "source": [
        "print(array(y1_train)[7])\n",
        "print(array(y2_train)[7])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iGpdOMvXRTD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4cbe41a4-8b04-489c-93a7-252e76fcafff"
      },
      "source": [
        "print(X2_train.shape)\n",
        "print(X2_val.shape)\n",
        "print(X2_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(62079, 65)\n",
            "(15520, 65)\n",
            "(19400, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BE9B_ecXXDl"
      },
      "source": [
        "#import all necessary package\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#matplotlib inlineimport keras\n",
        "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
        "from keras.models import Model,Sequential\n",
        "from keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvOt6rW1Xgt4"
      },
      "source": [
        "#define the optimizers\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "def adam_optimizer():\n",
        "    return Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "def RMSprop_optimizer():\n",
        "    return RMSprop(lr=0.001, rho=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvPehA6PXp_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "721c7051-31a1-42b3-9c89-914d3ba563ae"
      },
      "source": [
        "#Construct a classifier for the initial experiments\n",
        "\n",
        "def create_classifier(release=False,Tuser=195):\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Dense(128, input_dim=65))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  #classifier.add(Dense(256))\n",
        "  #classifier.add(BatchNormalization())\n",
        "  #classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(256))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(256))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  #classifier.add(Dense(256))\n",
        "  #classifier.add(BatchNormalization())\n",
        "  #classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(128))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  #if release:\n",
        "  classifier.add(Dense(Tuser, activation='softmax'))\n",
        "  #else:\n",
        "  #   classifier.add(Dense(Tuser))\n",
        "  #np.log_softmax_v2(a, axis=axis)\n",
        "  #classifier.add(F.softmax(a, dim=1))\n",
        "\n",
        "  classifier.compile(loss='categorical_crossentropy', optimizer=RMSprop_optimizer(),metrics=['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "Clasf=create_classifier()\n",
        "Clasf.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               8448      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 195)               25155     \n",
            "=================================================================\n",
            "Total params: 168,387\n",
            "Trainable params: 166,851\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REoN2JVcX6aJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8466279c-6dca-40fa-a869-16b2f8d1dd6e"
      },
      "source": [
        "#Train the classifier seperately for black-box attack\n",
        "import keras\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
        "callbacks_list = [learning_rate_reduction]\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
        "callbacks_list = [learning_rate_reduction]\n",
        "\n",
        "Classfier2= create_classifier(True,97)\n",
        "\n",
        "#------Comment will start from here\n",
        "lossc='categorical_crossentropy'\n",
        "optimizerc=RMSprop(lr=0.001, rho=0.9)\n",
        "Classfier2.compile(loss=lossc, optimizer=optimizerc,metrics=['accuracy'])\n",
        "#------Comments will end from here\n",
        "historyc2 =  Classfier2.fit(X1_train, y1_train, batch_size=64, epochs=50, validation_data=(X1_val, y1_val),verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 62079 samples, validate on 15520 samples\n",
            "Epoch 1/50\n",
            "62079/62079 [==============================] - 9s 140us/step - loss: 1.5521 - accuracy: 0.4509 - val_loss: 1.1123 - val_accuracy: 0.5604\n",
            "Epoch 2/50\n",
            " 1600/62079 [..............................] - ETA: 6s - loss: 1.0711 - accuracy: 0.5713"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.9408 - accuracy: 0.6289 - val_loss: 0.9003 - val_accuracy: 0.6545\n",
            "Epoch 3/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.6815 - accuracy: 0.7387 - val_loss: 0.6384 - val_accuracy: 0.7503\n",
            "Epoch 4/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.5162 - accuracy: 0.8063 - val_loss: 0.6306 - val_accuracy: 0.7613\n",
            "Epoch 5/50\n",
            "62079/62079 [==============================] - 8s 126us/step - loss: 0.4014 - accuracy: 0.8524 - val_loss: 0.4145 - val_accuracy: 0.8466\n",
            "Epoch 6/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.3305 - accuracy: 0.8806 - val_loss: 0.5391 - val_accuracy: 0.8203\n",
            "Epoch 7/50\n",
            "62079/62079 [==============================] - 8s 126us/step - loss: 0.2773 - accuracy: 0.8989 - val_loss: 0.4408 - val_accuracy: 0.8491\n",
            "Epoch 8/50\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.2433 - accuracy: 0.9126 - val_loss: 0.2870 - val_accuracy: 0.8967\n",
            "Epoch 9/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.2131 - accuracy: 0.9240 - val_loss: 0.2348 - val_accuracy: 0.9179\n",
            "Epoch 10/50\n",
            "62079/62079 [==============================] - 8s 126us/step - loss: 0.1961 - accuracy: 0.9295 - val_loss: 0.2170 - val_accuracy: 0.9218\n",
            "Epoch 11/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.1766 - accuracy: 0.9362 - val_loss: 0.2445 - val_accuracy: 0.9146\n",
            "Epoch 12/50\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.1615 - accuracy: 0.9417 - val_loss: 0.2975 - val_accuracy: 0.9052\n",
            "Epoch 13/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.1557 - accuracy: 0.9442 - val_loss: 0.2265 - val_accuracy: 0.9231\n",
            "Epoch 14/50\n",
            "62079/62079 [==============================] - 8s 129us/step - loss: 0.1452 - accuracy: 0.9489 - val_loss: 0.1603 - val_accuracy: 0.9459\n",
            "Epoch 15/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.1352 - accuracy: 0.9529 - val_loss: 0.2050 - val_accuracy: 0.9260\n",
            "Epoch 16/50\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.1272 - accuracy: 0.9551 - val_loss: 0.1559 - val_accuracy: 0.9468\n",
            "Epoch 17/50\n",
            "62079/62079 [==============================] - 8s 126us/step - loss: 0.1253 - accuracy: 0.9562 - val_loss: 0.1524 - val_accuracy: 0.9503\n",
            "Epoch 18/50\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.1218 - accuracy: 0.9576 - val_loss: 0.2004 - val_accuracy: 0.9365\n",
            "Epoch 19/50\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.1157 - accuracy: 0.9596 - val_loss: 0.1553 - val_accuracy: 0.9512\n",
            "Epoch 20/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.1078 - accuracy: 0.9627 - val_loss: 0.1897 - val_accuracy: 0.9409\n",
            "Epoch 21/50\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.1063 - accuracy: 0.9638 - val_loss: 0.1233 - val_accuracy: 0.9607\n",
            "Epoch 22/50\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0968 - accuracy: 0.9661 - val_loss: 0.0863 - val_accuracy: 0.9710\n",
            "Epoch 23/50\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0978 - accuracy: 0.9667 - val_loss: 0.1580 - val_accuracy: 0.9506\n",
            "Epoch 24/50\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0914 - accuracy: 0.9682 - val_loss: 0.1700 - val_accuracy: 0.9483\n",
            "Epoch 25/50\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0906 - accuracy: 0.9683 - val_loss: 0.1127 - val_accuracy: 0.9626\n",
            "Epoch 26/50\n",
            "62079/62079 [==============================] - 8s 129us/step - loss: 0.0891 - accuracy: 0.9693 - val_loss: 0.1025 - val_accuracy: 0.9684\n",
            "Epoch 27/50\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0853 - accuracy: 0.9703 - val_loss: 0.1175 - val_accuracy: 0.9640\n",
            "Epoch 28/50\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0862 - accuracy: 0.9708 - val_loss: 0.1072 - val_accuracy: 0.9667\n",
            "Epoch 29/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.0816 - accuracy: 0.9720 - val_loss: 0.1426 - val_accuracy: 0.9617\n",
            "Epoch 30/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.0796 - accuracy: 0.9724 - val_loss: 0.0804 - val_accuracy: 0.9760\n",
            "Epoch 31/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.0783 - accuracy: 0.9732 - val_loss: 0.1760 - val_accuracy: 0.9452\n",
            "Epoch 32/50\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0742 - accuracy: 0.9751 - val_loss: 0.1318 - val_accuracy: 0.9633\n",
            "Epoch 33/50\n",
            "62079/62079 [==============================] - 8s 123us/step - loss: 0.0766 - accuracy: 0.9745 - val_loss: 0.1197 - val_accuracy: 0.9615\n",
            "Epoch 34/50\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0765 - accuracy: 0.9737 - val_loss: 0.1103 - val_accuracy: 0.9684\n",
            "Epoch 35/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.0713 - accuracy: 0.9762 - val_loss: 0.0685 - val_accuracy: 0.9783\n",
            "Epoch 36/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.0725 - accuracy: 0.9751 - val_loss: 0.0845 - val_accuracy: 0.9745\n",
            "Epoch 37/50\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0721 - accuracy: 0.9759 - val_loss: 0.0592 - val_accuracy: 0.9806\n",
            "Epoch 38/50\n",
            "62079/62079 [==============================] - 8s 123us/step - loss: 0.0708 - accuracy: 0.9762 - val_loss: 0.0707 - val_accuracy: 0.9767\n",
            "Epoch 39/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.0685 - accuracy: 0.9773 - val_loss: 0.1175 - val_accuracy: 0.9646\n",
            "Epoch 40/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.0675 - accuracy: 0.9773 - val_loss: 0.0747 - val_accuracy: 0.9767\n",
            "Epoch 41/50\n",
            "62079/62079 [==============================] - 8s 123us/step - loss: 0.0646 - accuracy: 0.9790 - val_loss: 0.0730 - val_accuracy: 0.9786\n",
            "Epoch 42/50\n",
            "62079/62079 [==============================] - 8s 123us/step - loss: 0.0613 - accuracy: 0.9793 - val_loss: 0.0628 - val_accuracy: 0.9796\n",
            "Epoch 43/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.0642 - accuracy: 0.9786 - val_loss: 0.0565 - val_accuracy: 0.9822\n",
            "Epoch 44/50\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.0598 - accuracy: 0.9794 - val_loss: 0.1058 - val_accuracy: 0.9693\n",
            "Epoch 45/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.0587 - accuracy: 0.9798 - val_loss: 0.0674 - val_accuracy: 0.9769\n",
            "Epoch 46/50\n",
            "62079/62079 [==============================] - 8s 123us/step - loss: 0.0608 - accuracy: 0.9803 - val_loss: 0.0806 - val_accuracy: 0.9771\n",
            "Epoch 47/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.0616 - accuracy: 0.9790 - val_loss: 0.0616 - val_accuracy: 0.9823\n",
            "Epoch 48/50\n",
            "62079/62079 [==============================] - 8s 124us/step - loss: 0.0600 - accuracy: 0.9803 - val_loss: 0.0787 - val_accuracy: 0.9782\n",
            "Epoch 49/50\n",
            "62079/62079 [==============================] - 8s 122us/step - loss: 0.0597 - accuracy: 0.9794 - val_loss: 0.0530 - val_accuracy: 0.9829\n",
            "Epoch 50/50\n",
            "62079/62079 [==============================] - 8s 126us/step - loss: 0.0577 - accuracy: 0.9805 - val_loss: 0.0760 - val_accuracy: 0.9792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIDuDQkhguPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "7e7b23b5-04cb-4547-d4b0-b05dab5979d7"
      },
      "source": [
        "# Plot the classifier loss and accuracy curves for training and validation data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(historyc2.history['accuracy'], color='b', label=\"Training accuracy\")\n",
        "plt.plot(historyc2.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "legend = plt.legend(loc='best', shadow=True)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epochs')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epochs')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1OBjFl6YClq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1731274c-7b16-4953-8b57-b0ad33e37a3f"
      },
      "source": [
        "#Performance of the classifier\n",
        "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
        "loss, accuracy = Classfier2.evaluate(X1_test, y1_test)\n",
        "#print('Test score:', score)\n",
        "print('Loss:', loss)\n",
        "print('Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19400/19400 [==============================] - 1s 63us/step\n",
            "Loss: 0.06950189665262348\n",
            "Accuracy: 0.9777835011482239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6_nmsiIi1jq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8f6b10cf-9122-4509-a86f-3f9dceab65d3"
      },
      "source": [
        "#[Performance] of the classifier for first user group\n",
        "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
        "loss, accuracy = Classfier2.evaluate(X2_train, y2_train)\n",
        "#print('Test score:', score)\n",
        "print('Loss:', loss)\n",
        "print('Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62079/62079 [==============================] - 4s 60us/step\n",
            "Loss: 44.73250299755235\n",
            "Accuracy: 0.010921567678451538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC5E9bAcYAWy"
      },
      "source": [
        "#Input the data of [second users' group] to the classifier\n",
        "#Take only [top-ranked probability class index]\n",
        "import numpy as np\n",
        "rank=5\n",
        "X22_train=Classfier2.predict(X2_train)\n",
        "X22_test=Classfier2.predict(X2_test)\n",
        "n1,m1=X22_train.shape\n",
        "n2,m2=X22_test.shape\n",
        "\n",
        "y22_train = np.zeros([n1,5], dtype=float)\n",
        "y22_test = np.zeros([n2], dtype=float)\n",
        "\n",
        "\n",
        "for i in range(n1):\n",
        "  ind=X22_train[i].argsort()[-rank:][::-1]\n",
        "  y22_train[i][0]=ind[0]\n",
        "\n",
        "for i in range(n2):\n",
        "  ind=X22_test[i].argsort()[-rank:][::-1]\n",
        "  y22_test[i]=ind[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PgVzzpiB2O4"
      },
      "source": [
        "#Code for file read\n",
        "with open('Data/allSortedIndx.csv') as csvfile:\n",
        "    allSort2 = list(csv.reader(csvfile, delimiter=','))\n",
        "#allSort2=np.array(allSort2)\n",
        "#n1=allSort2.size\n",
        "#allSort22=np.zeros(int(n1/2))\n",
        "#indx=0\n",
        "#for i in range(n1):\n",
        "#  if(i%2==1):\n",
        "    #list.remove(allSort2[i])\n",
        "#     numpy.delete(allSort2,i)\n",
        "     #indx=indx+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOC-jMjJZ975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8728ede9-c70d-4dc1-909b-79cff392d096"
      },
      "source": [
        "#Based on top-ranked probability value [group all test data in different classes]\n",
        "#Based on test data class and top-ranked probablity class of tranning sample\n",
        "#calculate neighbor of top-ranked probablity class by using Euclidean distance based approach\n",
        "\n",
        "from scipy.spatial import distance\n",
        "import numpy\n",
        "\n",
        "n1,m1=X2_train.shape\n",
        "n2,m2=X2_test.shape\n",
        "\n",
        "y2T=array(y2T)\n",
        "y2Te=array(y2Te)\n",
        "X2_train=array(X2_train)\n",
        "X2_test=array(X2_test)\n",
        "kNN=250\n",
        "count=0;\n",
        "\n",
        "#indx=2\n",
        "sum=numpy.zeros(97)\n",
        "allSort2=numpy.zeros((n1,97))\n",
        "\n",
        "#distance.euclidean(X2_train[1].values, X2_test[1].values)\n",
        "#print('Hello')\n",
        "#with open('Data/allDistanceT.csv') as csvfile:\n",
        "#    d = list(csv.reader(csvfile, delimiter=','))\n",
        "\n",
        "for indx in range(5):\n",
        "  d=numpy.zeros(n2)\n",
        "  for j in range(n2):\n",
        "    #d[j] = distance.euclidean(encoded_train[j], encoded_test[indx])\n",
        "    d[j] = distance.euclidean(X2_train[indx].astype(float), X2_test[j].astype(float))\n",
        "\n",
        "  ind=d.argsort()\n",
        "\n",
        "  #classID=23\n",
        "  allSum=0\n",
        "  for classID in range(97): \n",
        "    sum1=0\n",
        "    sum2=0\n",
        "    k=0\n",
        "    l=0\n",
        "    while(k<kNN):\n",
        "    #print(encoded_test[i])\n",
        "      if(y22_test[ind[l]]==classID and y22_test[ind[l]]!=y22_train[indx][0]):\n",
        "        #d = distance.euclidean(encoded_train[ind[k]], encoded_test[indx])\n",
        "        d1 = d[ind[l]]\n",
        "        #d = distance.euclidean(X2_train[ind[k]].astype(float), X2_test[indx].astype(float))\n",
        "        sum1=sum1+np.exp(-d1)\n",
        "        k=k+1\n",
        "        l=l+1\n",
        "      elif(y22_test[ind[l]]!=y22_train[indx][0]) :\n",
        "        #d = distance.euclidean(encoded_train[ind[k]], encoded_test[indx])\n",
        "        d2 = d[ind[l]]\n",
        "        #d = distance.euclidean(X2_train[ind[k]].astype(float), X2_test[indx].astype(float))\n",
        "        sum2=sum2+np.exp(-d2)\n",
        "        k=k+1\n",
        "        l=l+1\n",
        "      else:\n",
        "        l=l+1\n",
        "\n",
        "    if(classID!=y22_train[indx][0]):\n",
        "      sum[classID]=sum1/(sum1+sum2)\n",
        "    else:\n",
        "     sum[classID]=-1\n",
        " \n",
        "  #allSum=allSum+(sum1/(sum1+sum2))\n",
        "  sumsort=sum.argsort()\n",
        "  allSort2[indx]=sumsort\n",
        "  #print(sumsort)\n",
        "  \n",
        "  if(y22_train[indx][0]==sumsort[96]):\n",
        "     count=count+1;\n",
        " #print(y2Te[indx])\n",
        " #print(count)\n",
        "\n",
        "print((count/1000)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbVtrvx5awqF"
      },
      "source": [
        "# writing to csv file  \n",
        "with open('Data/allSortedIndx1.csv', 'w') as csvfile:  \n",
        "    # creating a csv writer object  \n",
        "    csvwriter = csv.writer(csvfile)          \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(allSort2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5cxqucWCYfv"
      },
      "source": [
        "X22_train=Classfier2.predict(X2_train)\n",
        "n1,m1=X22_train.shape\n",
        "allSort2=np.array(allSort2)\n",
        "n2=allSort2.size\n",
        "\n",
        "rank=97\n",
        "orgNeighbor=np.zeros([n1,97], dtype=float)\n",
        "predNeighbor=np.zeros([n1,97], dtype=float)\n",
        "\n",
        "orgNeighbor1=np.zeros([n1,5], dtype=float)\n",
        "predNeighbor1=np.zeros([n1,5], dtype=float)\n",
        "\n",
        "orgNeighbor2=np.zeros([n1,10], dtype=float)\n",
        "predNeighbor2=np.zeros([n1,10], dtype=float)\n",
        "\n",
        "for i in range(n2):\n",
        "  if(i%2!=1):\n",
        "    predNeighbor[int(i/2)]=allSort2[i]\n",
        "\n",
        "\n",
        "for i in range(n1):\n",
        "  orgNeighbor[i]=X22_train[i].argsort()[-rank:][::-1]\n",
        "\n",
        "for i in range(n1):\n",
        "   for j in range(1,6):\n",
        "    orgNeighbor1[i][j-1]=orgNeighbor[i][j]\n",
        "    predNeighbor1[i][j-1]=predNeighbor[i][97-j]\n",
        "\n",
        "for i in range(n1):\n",
        "   for j in range(1,11):\n",
        "    orgNeighbor2[i][j-1]=orgNeighbor[i][j]\n",
        "    predNeighbor2[i][j-1]=predNeighbor[i][97-j]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pMhOINla0X9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7ac935d9-7e6f-4565-b9d6-a96f28e955e4"
      },
      "source": [
        "# writing to csv file  \n",
        "with open('Data/orgNeighborT.csv', 'w') as csvfile:  \n",
        "    # creating a csv writer object  \n",
        "    csvwriter = csv.writer(csvfile)          \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(orgNeighbor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['8.0', '0.0', '70.0', '69.0', '68.0', '67.0', '66.0', '65.0', '64.0', '63.0', '62.0', '61.0', '60.0', '59.0', '58.0', '57.0', '56.0', '55.0', '54.0', '53.0', '52.0', '51.0', '71.0', '72.0', '73.0', '74.0', '94.0', '93.0', '92.0', '91.0', '90.0', '89.0', '88.0', '87.0', '86.0', '50.0', '85.0', '83.0', '82.0', '81.0', '80.0', '79.0', '78.0', '77.0', '76.0', '75.0', '84.0', '49.0', '48.0', '26.0', '95.0', '22.0', '21.0', '20.0', '19.0', '18.0', '17.0', '16.0', '15.0', '14.0', '13.0', '12.0', '11.0', '9.0', '7.0', '6.0', '5.0', '25.0', '4.0', '3.0', '28.0', '1.0', '44.0', '43.0', '42.0', '41.0', '40.0', '39.0', '27.0', '38.0', '34.0', '33.0', '32.0', '31.0', '30.0', '29.0', '2.0', '96.0', '10.0', '45.0', '37.0', '23.0', '35.0', '24.0', '47.0', '46.0', '36.0']\n",
            "36.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHsMWhY_F-30"
      },
      "source": [
        "commonSize1=np.zeros(n1)\n",
        "commonSize2=np.zeros(n1)\n",
        "\n",
        "for i in range(n1):\n",
        "  orgNeighborSet1=set(orgNeighbor1[i])\n",
        "  predNeighborSet1=set(predNeighbor1[i])\n",
        "  common1 = orgNeighborSet1.intersection(predNeighborSet1)\n",
        "  commonSize1[i]=len(common1)\n",
        "\n",
        "  orgNeighborSet2=set(orgNeighbor2[i])\n",
        "  predNeighborSet2=set(predNeighbor2[i])\n",
        "  common2 = orgNeighborSet2.intersection(predNeighborSet2)\n",
        "  commonSize2[i]=len(common2)\n",
        "\n",
        "#print(commonSize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEkhHLK3GFrS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "5293b2ab-3cad-4159-c3cc-c4de607f975b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "X=np.arange(0, n1, 1).tolist()\n",
        "xlocs, xlabs = plt.xticks()\n",
        "xlabs=[i for i in range(0,10)]\n",
        "x_pos = [i for i, _ in enumerate(xlabs)] \n",
        "Y1=commonSize1\n",
        "Y2=commonSize2\n",
        "plt.hist([Y1,Y2])\n",
        "plt.legend(['Top 5 neighbor classes','Top 10 neighbor classes'])\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Common class')\n",
        "plt.xlim(0,10)\n",
        "#plt.ylim(0,11500)\n",
        "plt.xticks(x_pos, xlabs)\n",
        "#legend = plt.legend(loc='best', shadow=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4PWqkVdUTrV"
      },
      "source": [
        "#Code for file read\n",
        "#Take 4 neighbor of top-ranked class\n",
        "allSort2=np.array(allSort2)\n",
        "n1=allSort2.size\n",
        "for i in range(n1):\n",
        "  if(i%2!=1):\n",
        "    y22_train[int(i/2)][1]=allSort2[i][96]\n",
        "    y22_train[int(i/2)][2]=allSort2[i][95]\n",
        "    y22_train[int(i/2)][3]=allSort2[i][94]\n",
        "    y22_train[int(i/2)][4]=allSort2[i][93]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScNTWHgda7Hu"
      },
      "source": [
        "#Take 4 neighbor of top-ranked class\n",
        "for i in range(n1):\n",
        "  y22_train[i][1]=allSort2[i][96]\n",
        "  y22_train[i][2]=allSort2[i][95]\n",
        "  y22_train[i][3]=allSort2[i][94]\n",
        "  y22_train[i][4]=allSort2[i][93]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTs-dvT3bJsd"
      },
      "source": [
        "#Fillup the table by 4 different probability based on neighborness\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "rank=5\n",
        "n,m=X22_train.shape\n",
        "X2_InvTrain = np.zeros([n, m], dtype=float)\n",
        "for i in range(n):\n",
        " sumP=0\n",
        " start=0\n",
        " end=1\n",
        " for j in range(rank):\n",
        "  if(j==0):\n",
        "    X2_InvTrain[i,int(y22_train[i][j])]= X22_train[i, int(y22_train[i][j])]\n",
        "    sumP+=X22_train[i, int(y22_train[i][j])]\n",
        "  else:\n",
        "    X2_InvTrain[i,int(y22_train[i][j])]=random.uniform(start, end)\n",
        "    sumP+=X2_InvTrain[i,int(y22_train[i][j])]\n",
        "    #print(X2_InvTrain[i,int(y22_train[i][j])])\n",
        "  remaining=1-sumP\n",
        "  #remaining=1-sum(X2_InvTrain[i])\n",
        "  end=remaining\n",
        "    #print(end)\n",
        "  if(end >=0.1):\n",
        "   start=end-0.1\n",
        "  else:\n",
        "   start=0\n",
        "    #print(indx)\n",
        "    \n",
        "    #print(y1_Rand[i,indx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbdpDZBqbx1P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "8f33ad6c-bed5-463d-b470-54f8d7b87246"
      },
      "source": [
        "#print one of the probablity\n",
        "print(X2_InvTrain[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 2.51944401e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 2.73829770e-04 9.99144197e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 1.96737242e-04 8.51332642e-05\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVB9mn1ab0g-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "b2d361a0-95c6-43e9-b237-c0d02af4d0c4"
      },
      "source": [
        "#Create a inverse model for [black-box] attacks\n",
        "\n",
        "def create_Invclassifier(Tuser=195):\n",
        "  Invclassifier = Sequential()\n",
        "  Invclassifier.add(Dense(128, input_dim=Tuser))\n",
        "  Invclassifier.add(BatchNormalization())\n",
        "  Invclassifier.add(Activation('tanh'))\n",
        "\n",
        "  #Invclassifier.add(Dense(256))\n",
        "  #Invclassifier.add(BatchNormalization())\n",
        "  #Invclassifier.add(Activation('tanh'))\n",
        "\n",
        "  Invclassifier.add(Dense(256))\n",
        "  Invclassifier.add(BatchNormalization())\n",
        "  Invclassifier.add(Activation('tanh'))\n",
        "\n",
        "  Invclassifier.add(Dense(256))\n",
        "  Invclassifier.add(BatchNormalization())\n",
        "  Invclassifier.add(Activation('tanh'))\n",
        "\n",
        "  #Invclassifier.add(Dense(256))\n",
        "  #Invclassifier.add(BatchNormalization())\n",
        "  #Invclassifier.add(Activation('tanh'))\n",
        "\n",
        "  Invclassifier.add(Dense(128))\n",
        "  Invclassifier.add(BatchNormalization())\n",
        "  Invclassifier.add(Activation('tanh'))\n",
        "\n",
        "  Invclassifier.add(Dense(65,activation='sigmoid'))\n",
        "\n",
        "  #Invclassifier.compile(loss='mse', optimizer=adam_optimizer(),metrics=['accuracy'])\n",
        "  return Invclassifier\n",
        "\n",
        "InvClsf=create_Invclassifier()\n",
        "InvClsf.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 128)               25088     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 65)                8385      \n",
            "=================================================================\n",
            "Total params: 168,257\n",
            "Trainable params: 166,721\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rL9uA-Bb9rg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53152073-994e-4190-a467-5ef5ee888c8a"
      },
      "source": [
        "#Train the inverse classifier seperately for black-box attack\n",
        "import keras\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
        "callbacks_list = [learning_rate_reduction]\n",
        "\n",
        "InvClsfI1=create_Invclassifier(97)\n",
        "\n",
        "#------Comment will start from here\n",
        "lossc='mse'\n",
        "optimizerc=RMSprop(lr=0.001, rho=0.9)\n",
        "InvClsfI1.compile(loss=lossc, optimizer=optimizerc)\n",
        "#------Comments will end from here\n",
        "historyc2 =  InvClsfI1.fit(X2_InvTrain, X2_train, batch_size=64, epochs=200,verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62079/62079 [==============================] - 9s 138us/step - loss: 0.0289\n",
            "Epoch 2/200\n",
            " 1344/62079 [..............................] - ETA: 7s - loss: 0.0230"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0220\n",
            "Epoch 3/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0208\n",
            "Epoch 4/200\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0200\n",
            "Epoch 5/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0194\n",
            "Epoch 6/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0188\n",
            "Epoch 7/200\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0185\n",
            "Epoch 8/200\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0180\n",
            "Epoch 9/200\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0177\n",
            "Epoch 10/200\n",
            "62079/62079 [==============================] - 8s 129us/step - loss: 0.0175\n",
            "Epoch 11/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0173\n",
            "Epoch 12/200\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0170\n",
            "Epoch 13/200\n",
            "62079/62079 [==============================] - 8s 129us/step - loss: 0.0169\n",
            "Epoch 14/200\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0167\n",
            "Epoch 15/200\n",
            "62079/62079 [==============================] - 8s 129us/step - loss: 0.0165\n",
            "Epoch 16/200\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0163\n",
            "Epoch 17/200\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0162\n",
            "Epoch 18/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0161\n",
            "Epoch 19/200\n",
            "62079/62079 [==============================] - 8s 123us/step - loss: 0.0159\n",
            "Epoch 20/200\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0158\n",
            "Epoch 21/200\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.0156\n",
            "Epoch 22/200\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0155\n",
            "Epoch 23/200\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0155\n",
            "Epoch 24/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0153\n",
            "Epoch 25/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0152\n",
            "Epoch 26/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0151\n",
            "Epoch 27/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0150\n",
            "Epoch 28/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0149\n",
            "Epoch 29/200\n",
            "62079/62079 [==============================] - 8s 136us/step - loss: 0.0148\n",
            "Epoch 30/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0147\n",
            "Epoch 31/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0146\n",
            "Epoch 32/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0145\n",
            "Epoch 33/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0144\n",
            "Epoch 34/200\n",
            "62079/62079 [==============================] - 8s 129us/step - loss: 0.0144\n",
            "Epoch 35/200\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0143\n",
            "Epoch 36/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0142\n",
            "Epoch 37/200\n",
            "62079/62079 [==============================] - 8s 129us/step - loss: 0.0141\n",
            "Epoch 38/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0140\n",
            "Epoch 39/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0140\n",
            "Epoch 40/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0139\n",
            "Epoch 41/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0139\n",
            "Epoch 42/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0138\n",
            "Epoch 43/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0138\n",
            "Epoch 44/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0137\n",
            "Epoch 45/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0137\n",
            "Epoch 46/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0136\n",
            "Epoch 47/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0135\n",
            "Epoch 48/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0135\n",
            "Epoch 49/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0135\n",
            "Epoch 50/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0134\n",
            "Epoch 51/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0134\n",
            "Epoch 52/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0134\n",
            "Epoch 53/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0134\n",
            "Epoch 54/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0134\n",
            "Epoch 55/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0132\n",
            "Epoch 56/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0132\n",
            "Epoch 57/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0132\n",
            "Epoch 58/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0131\n",
            "Epoch 59/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0131\n",
            "Epoch 60/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0132\n",
            "Epoch 61/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0130\n",
            "Epoch 62/200\n",
            "62079/62079 [==============================] - 8s 136us/step - loss: 0.0131\n",
            "Epoch 63/200\n",
            "62079/62079 [==============================] - 8s 136us/step - loss: 0.0130\n",
            "Epoch 64/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0130\n",
            "Epoch 65/200\n",
            "62079/62079 [==============================] - 8s 125us/step - loss: 0.0130\n",
            "Epoch 66/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0129\n",
            "Epoch 67/200\n",
            "62079/62079 [==============================] - 8s 127us/step - loss: 0.0129\n",
            "Epoch 68/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0129\n",
            "Epoch 69/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0128\n",
            "Epoch 70/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0128\n",
            "Epoch 71/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0128\n",
            "Epoch 72/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0128\n",
            "Epoch 73/200\n",
            "62079/62079 [==============================] - 11s 178us/step - loss: 0.0127\n",
            "Epoch 74/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0127\n",
            "Epoch 75/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0127\n",
            "Epoch 76/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0127\n",
            "Epoch 77/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0126\n",
            "Epoch 78/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0126\n",
            "Epoch 79/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0126\n",
            "Epoch 80/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0126\n",
            "Epoch 81/200\n",
            "62079/62079 [==============================] - 8s 128us/step - loss: 0.0126\n",
            "Epoch 82/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0125\n",
            "Epoch 83/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0125\n",
            "Epoch 84/200\n",
            "62079/62079 [==============================] - 8s 137us/step - loss: 0.0125\n",
            "Epoch 85/200\n",
            "62079/62079 [==============================] - 9s 143us/step - loss: 0.0125\n",
            "Epoch 86/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0125\n",
            "Epoch 87/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0125\n",
            "Epoch 88/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0124\n",
            "Epoch 89/200\n",
            "62079/62079 [==============================] - 9s 147us/step - loss: 0.0124\n",
            "Epoch 90/200\n",
            "62079/62079 [==============================] - 9s 144us/step - loss: 0.0124\n",
            "Epoch 91/200\n",
            "62079/62079 [==============================] - 9s 144us/step - loss: 0.0124\n",
            "Epoch 92/200\n",
            "62079/62079 [==============================] - 9s 143us/step - loss: 0.0124\n",
            "Epoch 93/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0123\n",
            "Epoch 94/200\n",
            "62079/62079 [==============================] - 9s 143us/step - loss: 0.0123\n",
            "Epoch 95/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0123\n",
            "Epoch 96/200\n",
            "62079/62079 [==============================] - 8s 136us/step - loss: 0.0123\n",
            "Epoch 97/200\n",
            "62079/62079 [==============================] - 9s 137us/step - loss: 0.0123\n",
            "Epoch 98/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0123\n",
            "Epoch 99/200\n",
            "62079/62079 [==============================] - 9s 148us/step - loss: 0.0122\n",
            "Epoch 100/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0122\n",
            "Epoch 101/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0122\n",
            "Epoch 102/200\n",
            "62079/62079 [==============================] - 9s 138us/step - loss: 0.0122\n",
            "Epoch 103/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0121\n",
            "Epoch 104/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0121\n",
            "Epoch 105/200\n",
            "62079/62079 [==============================] - 9s 141us/step - loss: 0.0121\n",
            "Epoch 106/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0121\n",
            "Epoch 107/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0121\n",
            "Epoch 108/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0121\n",
            "Epoch 109/200\n",
            "62079/62079 [==============================] - 9s 145us/step - loss: 0.0121\n",
            "Epoch 110/200\n",
            "62079/62079 [==============================] - 9s 138us/step - loss: 0.0121\n",
            "Epoch 111/200\n",
            "62079/62079 [==============================] - 9s 143us/step - loss: 0.0120\n",
            "Epoch 112/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0120\n",
            "Epoch 113/200\n",
            "62079/62079 [==============================] - 9s 148us/step - loss: 0.0121\n",
            "Epoch 114/200\n",
            "62079/62079 [==============================] - 9s 148us/step - loss: 0.0120\n",
            "Epoch 115/200\n",
            "62079/62079 [==============================] - 9s 150us/step - loss: 0.0120\n",
            "Epoch 116/200\n",
            "62079/62079 [==============================] - 9s 151us/step - loss: 0.0120\n",
            "Epoch 117/200\n",
            "62079/62079 [==============================] - 9s 150us/step - loss: 0.0120\n",
            "Epoch 118/200\n",
            "62079/62079 [==============================] - 9s 150us/step - loss: 0.0119\n",
            "Epoch 119/200\n",
            "62079/62079 [==============================] - 10s 153us/step - loss: 0.0119\n",
            "Epoch 120/200\n",
            "62079/62079 [==============================] - 10s 157us/step - loss: 0.0120\n",
            "Epoch 121/200\n",
            "62079/62079 [==============================] - 9s 152us/step - loss: 0.0119\n",
            "Epoch 122/200\n",
            "62079/62079 [==============================] - 9s 147us/step - loss: 0.0119\n",
            "Epoch 123/200\n",
            "62079/62079 [==============================] - 9s 149us/step - loss: 0.0120\n",
            "Epoch 124/200\n",
            "62079/62079 [==============================] - 9s 150us/step - loss: 0.0119\n",
            "Epoch 125/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0118\n",
            "Epoch 126/200\n",
            "62079/62079 [==============================] - 8s 130us/step - loss: 0.0119\n",
            "Epoch 127/200\n",
            "62079/62079 [==============================] - 9s 145us/step - loss: 0.0118\n",
            "Epoch 128/200\n",
            "62079/62079 [==============================] - 9s 140us/step - loss: 0.0118\n",
            "Epoch 129/200\n",
            "62079/62079 [==============================] - 9s 146us/step - loss: 0.0118\n",
            "Epoch 130/200\n",
            "62079/62079 [==============================] - 9s 139us/step - loss: 0.0118\n",
            "Epoch 131/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0118\n",
            "Epoch 132/200\n",
            "62079/62079 [==============================] - 9s 141us/step - loss: 0.0118\n",
            "Epoch 133/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0117\n",
            "Epoch 134/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0117\n",
            "Epoch 135/200\n",
            "62079/62079 [==============================] - 8s 137us/step - loss: 0.0117\n",
            "Epoch 136/200\n",
            "62079/62079 [==============================] - 9s 137us/step - loss: 0.0117\n",
            "Epoch 137/200\n",
            "62079/62079 [==============================] - 8s 136us/step - loss: 0.0117\n",
            "Epoch 138/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0117\n",
            "Epoch 139/200\n",
            "62079/62079 [==============================] - 8s 137us/step - loss: 0.0117\n",
            "Epoch 140/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0116\n",
            "Epoch 141/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0116\n",
            "Epoch 142/200\n",
            "62079/62079 [==============================] - 11s 183us/step - loss: 0.0117\n",
            "Epoch 143/200\n",
            "62079/62079 [==============================] - 8s 137us/step - loss: 0.0116\n",
            "Epoch 144/200\n",
            "62079/62079 [==============================] - 9s 138us/step - loss: 0.0116\n",
            "Epoch 145/200\n",
            "62079/62079 [==============================] - 8s 137us/step - loss: 0.0116\n",
            "Epoch 146/200\n",
            "62079/62079 [==============================] - 8s 136us/step - loss: 0.0116\n",
            "Epoch 147/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0116\n",
            "Epoch 148/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0116\n",
            "Epoch 149/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0116\n",
            "Epoch 150/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0116\n",
            "Epoch 151/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0116\n",
            "Epoch 152/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0116\n",
            "Epoch 153/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0115\n",
            "Epoch 154/200\n",
            "62079/62079 [==============================] - 9s 138us/step - loss: 0.0116\n",
            "Epoch 155/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0115\n",
            "Epoch 156/200\n",
            "62079/62079 [==============================] - 8s 136us/step - loss: 0.0115\n",
            "Epoch 157/200\n",
            "62079/62079 [==============================] - 8s 136us/step - loss: 0.0115\n",
            "Epoch 158/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0115\n",
            "Epoch 159/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0115\n",
            "Epoch 160/200\n",
            "62079/62079 [==============================] - 9s 139us/step - loss: 0.0115\n",
            "Epoch 161/200\n",
            "62079/62079 [==============================] - 8s 132us/step - loss: 0.0115\n",
            "Epoch 162/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0115\n",
            "Epoch 163/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0114\n",
            "Epoch 164/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0115\n",
            "Epoch 165/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0115\n",
            "Epoch 166/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0115\n",
            "Epoch 167/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0115\n",
            "Epoch 168/200\n",
            "62079/62079 [==============================] - 10s 155us/step - loss: 0.0114\n",
            "Epoch 169/200\n",
            "62079/62079 [==============================] - 8s 133us/step - loss: 0.0114\n",
            "Epoch 170/200\n",
            "62079/62079 [==============================] - 8s 135us/step - loss: 0.0114\n",
            "Epoch 171/200\n",
            "62079/62079 [==============================] - 9s 140us/step - loss: 0.0114\n",
            "Epoch 172/200\n",
            "62079/62079 [==============================] - 8s 134us/step - loss: 0.0114\n",
            "Epoch 173/200\n",
            "62079/62079 [==============================] - 8s 131us/step - loss: 0.0114\n",
            "Epoch 174/200\n",
            "62079/62079 [==============================] - 9s 140us/step - loss: 0.0114\n",
            "Epoch 175/200\n",
            "62079/62079 [==============================] - 10s 156us/step - loss: 0.0114\n",
            "Epoch 176/200\n",
            "62079/62079 [==============================] - 9s 150us/step - loss: 0.0114\n",
            "Epoch 177/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0113\n",
            "Epoch 178/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0113\n",
            "Epoch 179/200\n",
            "62079/62079 [==============================] - 10s 153us/step - loss: 0.0114\n",
            "Epoch 180/200\n",
            "62079/62079 [==============================] - 10s 165us/step - loss: 0.0113\n",
            "Epoch 181/200\n",
            "62079/62079 [==============================] - 10s 168us/step - loss: 0.0113\n",
            "Epoch 182/200\n",
            "62079/62079 [==============================] - 10s 160us/step - loss: 0.0114\n",
            "Epoch 183/200\n",
            "62079/62079 [==============================] - 10s 159us/step - loss: 0.0113\n",
            "Epoch 184/200\n",
            "62079/62079 [==============================] - 9s 152us/step - loss: 0.0113\n",
            "Epoch 185/200\n",
            "62079/62079 [==============================] - 10s 159us/step - loss: 0.0113\n",
            "Epoch 186/200\n",
            "62079/62079 [==============================] - 9s 152us/step - loss: 0.0113\n",
            "Epoch 187/200\n",
            "62079/62079 [==============================] - 10s 160us/step - loss: 0.0113\n",
            "Epoch 188/200\n",
            "62079/62079 [==============================] - 10s 159us/step - loss: 0.0113\n",
            "Epoch 189/200\n",
            "62079/62079 [==============================] - 10s 159us/step - loss: 0.0113\n",
            "Epoch 190/200\n",
            "62079/62079 [==============================] - 10s 156us/step - loss: 0.0113\n",
            "Epoch 191/200\n",
            "62079/62079 [==============================] - 10s 158us/step - loss: 0.0112\n",
            "Epoch 192/200\n",
            "62079/62079 [==============================] - 10s 163us/step - loss: 0.0112\n",
            "Epoch 193/200\n",
            "62079/62079 [==============================] - 10s 159us/step - loss: 0.0112\n",
            "Epoch 194/200\n",
            "62079/62079 [==============================] - 9s 142us/step - loss: 0.0112\n",
            "Epoch 195/200\n",
            "62079/62079 [==============================] - 9s 147us/step - loss: 0.0111\n",
            "Epoch 196/200\n",
            "62079/62079 [==============================] - 10s 154us/step - loss: 0.0112\n",
            "Epoch 197/200\n",
            "62079/62079 [==============================] - 10s 163us/step - loss: 0.0112\n",
            "Epoch 198/200\n",
            "62079/62079 [==============================] - 10s 163us/step - loss: 0.0112\n",
            "Epoch 199/200\n",
            "62079/62079 [==============================] - 9s 151us/step - loss: 0.0112\n",
            "Epoch 200/200\n",
            "62079/62079 [==============================] - 9s 153us/step - loss: 0.0112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfcDdF8RnYy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "db345e2d-c9ea-4a4e-bee3-ceb881440153"
      },
      "source": [
        "# Plot the classifier loss and accuracy curves for training and validation data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(historyc2.history['loss'], color='b', label=\"Training loss\")\n",
        "#plt.plot(historyc3.history['val_loss'], color='r',label=\"Validation loss\")\n",
        "legend = plt.legend(loc='best', shadow=True)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epochs')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKdquk2ACGpz"
      },
      "source": [
        "#Code for file read\n",
        "with open('Data/RankTable.csv') as csvfile:\n",
        "    RankTable = list(csv.reader(csvfile, delimiter=','))\n",
        "RankTable = pd.DataFrame(RankTable[0:][:])\n",
        "#RankTable=np.array(RankTable)\n",
        "#print(RankTable)\n",
        "n1,m1=RankTable.shape\n",
        "for i in range(n1):\n",
        "  if (i%2!=0):\n",
        "    RankTable=RankTable.drop(i)\n",
        "RankTable.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_kIxhO6drwl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "969ed580-1ced-4105-f41a-c62611db0de3"
      },
      "source": [
        "print(RankTable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      0     1     2     3     4     5   ...    91    92    93    94    95    96\n",
            "0    0.0  69.0  68.0  67.0  66.0  65.0  ...  11.0  24.0  47.0  12.0  35.0  36.0\n",
            "1    0.0  69.0  68.0  67.0  66.0  65.0  ...  45.0  46.0  24.0  47.0  35.0  36.0\n",
            "2    0.0  43.0  44.0  95.0  51.0  52.0  ...  35.0  36.0  71.0  69.0  72.0  70.0\n",
            "3    0.0  61.0  60.0  59.0  58.0  57.0  ...  48.0  73.0  71.0  69.0  72.0  70.0\n",
            "4    0.0  35.0  36.0  41.0  43.0  44.0  ...  59.0  57.0  89.0  70.0  27.0  28.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "92   0.0  36.0  41.0  42.0  45.0  49.0  ...  59.0  30.0  57.0  51.0  50.0  52.0\n",
            "93   0.0  41.0  42.0  45.0  47.0  49.0  ...  30.0  59.0  57.0  51.0  50.0  52.0\n",
            "94   0.0  34.0  35.0  40.0  41.0  42.0  ...   6.0  29.0  62.0   9.0  30.0   8.0\n",
            "95  48.0  34.0  35.0  40.0  41.0  42.0  ...  28.0  29.0   9.0  62.0  30.0   8.0\n",
            "96  96.0  34.0  35.0  42.0  43.0  24.0  ...   7.0  29.0   9.0  62.0  30.0   8.0\n",
            "\n",
            "[97 rows x 97 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlWCv0CbeMl5"
      },
      "source": [
        "#Calculate the distance among test data\n",
        "from scipy.spatial import distance\n",
        "import numpy\n",
        "n1,m1=X2_train.shape\n",
        "n2,m2=X2_test.shape\n",
        "\n",
        "y2T=array(y2T)\n",
        "y2Te=array(y2Te)\n",
        "X2_train=array(X2_train)\n",
        "X2_test=array(X2_test)\n",
        "kNN=250\n",
        "d=numpy.zeros((n2,n2))\n",
        "for i in range(n2):\n",
        "  for j in range(n2):\n",
        "    d[i][j] = distance.euclidean(X2_test[i].astype(float), X2_test[j].astype(float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy8Dy89_eWKs"
      },
      "source": [
        "#Calculate the neighbor of original class based on auxilary profile\n",
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "#encoded_train = encoder.predict(X2_train)\n",
        "#encoded_test = encoder.predict(X2_test)\n",
        "\n",
        "n1,m1=X2_train.shape\n",
        "n2,m2=X2_test.shape\n",
        "\n",
        "y2T=array(y2T)\n",
        "y2Te=array(y2Te)\n",
        "X2_train=array(X2_train)\n",
        "X2_test=array(X2_test)\n",
        "kNN=250\n",
        "count=0;\n",
        "\n",
        "#indx=2\n",
        "\n",
        "allSort2=numpy.zeros((n1,97))\n",
        "\n",
        "n1,m1=d.shape\n",
        "unique_elements, counts_elements = np.unique(y2Te, return_counts=True)\n",
        "RankDistance=numpy.zeros((unique_elements.size,unique_elements.size))\n",
        "RankTable=numpy.zeros((unique_elements.size,unique_elements.size))\n",
        "\n",
        "for classID1 in unique_elements:\n",
        "   CID1index=numpy.zeros(counts_elements[classID1])\n",
        "   n2=CID1index.size\n",
        "   print(n2)\n",
        "   indx=0\n",
        "   for i in range(n1):\n",
        "     if(y2Te[i]==classID1):\n",
        "       CID1index[indx]=i\n",
        "       indx=indx+1\n",
        "\n",
        "   for classID2 in unique_elements:\n",
        "     sumPUser=0\n",
        "     #print(sumPUser)\n",
        "     for eachIndx in range(n2):\n",
        "       #print(CID1index[eachIndx])  \n",
        "       ind=d[int(CID1index[eachIndx])].argsort() \n",
        "       sum1=0\n",
        "       sum2=0 \n",
        "       k=0 \n",
        "       l=0 \n",
        "       while (k<kNN and l<n2):\n",
        "         if(y22_test[ind[l]]!=classID1 and y22_test[ind[l]]==classID2):\n",
        "           #d = distance.euclidean(encoded_train[ind[k]], encoded_test[indx])\n",
        "           d1 = d[int(CID1index[eachIndx])][ind[l]]\n",
        "           #d = distance.euclidean(X2_train[ind[k]].astype(float), X2_test[indx].astype(float))\n",
        "           sum1=sum1+np.exp(-d1)\n",
        "           k=k+1\n",
        "           l=l+1\n",
        "         elif(y22_test[ind[l]]!=classID1) :\n",
        "           #d = distance.euclidean(encoded_train[ind[k]], encoded_test[indx])\n",
        "           d2 = d[int(CID1index[eachIndx])][ind[l]]\n",
        "           #d = distance.euclidean(X2_train[ind[k]].astype(float), X2_test[indx].astype(float))\n",
        "           sum2=sum2+np.exp(-d2)\n",
        "           k=k+1\n",
        "           l=l+1\n",
        "         else:\n",
        "           l=l+1\n",
        "       \n",
        "       sumPUser+= (sum1/(sum1+sum2))\n",
        "       #print(sumPUser)\n",
        "     #print(classID2)\n",
        "     RankDistance[classID1][classID2]=sumPUser/n2\n",
        "     #print(sumPUser/n2)\n",
        "   RankTable[classID1]=RankDistance[classID1].argsort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZDcEKr_eXtZ"
      },
      "source": [
        "#Print rank-table\n",
        "print(RankTable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE23NDRzegRs"
      },
      "source": [
        "# writing to csv file  \n",
        "with open('Data/RankTable.csv', 'w') as csvfile:  \n",
        "    # creating a csv writer object  \n",
        "    csvwriter = csv.writer(csvfile)          \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(RankTable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iKG2-EtemlE"
      },
      "source": [
        "#Code for file read\n",
        "#Random generation of input for inverse classifier\n",
        "import numpy as np\n",
        "rank=10\n",
        "y22_train=Classfier2.predict(X1_train)\n",
        "y22_train=array(y22_train)\n",
        "n,m=y22_train.shape\n",
        "y1_Rand = np.zeros([n, m], dtype=float)\n",
        "#ind=y22_train[0].argsort()[-rank:][::-1]\n",
        "#indx=int(float(RankTable[ind[0]][96]))\n",
        "#print(indx)\n",
        "\n",
        "for i in range(n):\n",
        " ind=y22_train[i].argsort()[-rank:][::-1]\n",
        " start=0.98\n",
        " end=1.0\n",
        " sumP=0\n",
        " for j in range(rank):\n",
        "  if(j==0):\n",
        "    np.put(y1_Rand[i], ind[0], random.uniform(start, end))\n",
        "    sumP+=y1_Rand[i,ind[0]]\n",
        "    #print(y1_Rand[i])\n",
        "  else:\n",
        "    indx=int(float(RankTable[ind[0]][97-j]))\n",
        "    #indx=int(RankTable[ind[0],97-j])\n",
        "    y1_Rand[i,indx]=random.uniform(start, end)\n",
        "    sumP+=y1_Rand[i,indx]\n",
        "  remaining=1-sumP\n",
        "  end=remaining\n",
        "  #print(end)\n",
        "  if(end >=0.1):\n",
        "   start=end-0.1\n",
        "  else:\n",
        "   start=0  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r_uCpIckarU"
      },
      "source": [
        "#Random generation of input for inverse classifier\n",
        "import numpy as np\n",
        "rank=10\n",
        "y22_train=Classfier2.predict(X1_train)\n",
        "y22_train=array(y22_train)\n",
        "n,m=y22_train.shape\n",
        "y1_Rand = np.zeros([n, m], dtype=float)\n",
        "\n",
        "for i in range(n):\n",
        " ind=y22_train[i].argsort()[-rank:][::-1]\n",
        " start=0.98\n",
        " end=1.0\n",
        " sumP=0\n",
        " for j in range(rank):\n",
        "  if(j==0):\n",
        "    np.put(y1_Rand[i], ind[0], random.uniform(start, end))\n",
        "    sumP+=y1_Rand[i,ind[0]]\n",
        "    #print(y1_Rand[i])\n",
        "  else:\n",
        "    indx=int(RankTable[ind[0],97-j])\n",
        "    #indx=int(RankTable[ind[0],97-j])\n",
        "    y1_Rand[i,indx]=random.uniform(start, end)\n",
        "    sumP+=y1_Rand[i,indx]\n",
        "  remaining=1-sumP\n",
        "  end=remaining\n",
        "  #print(end)\n",
        "  if(end >=0.1):\n",
        "   start=end-0.1\n",
        "  else:\n",
        "   start=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjCA5aM2e4FD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "53ad94e4-16f0-4b95-b88f-c307c58407de"
      },
      "source": [
        "#print random generated input\n",
        "print(y1_Rand[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 2.64937560e-03 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 2.16830720e-05 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 9.94221689e-01 0.00000000e+00 0.00000000e+00 1.36741673e-06\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 3.13808093e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 3.35439435e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 8.84061032e-06 0.00000000e+00 0.00000000e+00 1.15964533e-03\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 1.31532680e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0afH4Rve7NE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "348f0ca2-7bae-40ea-9453-73de10df1a0f"
      },
      "source": [
        "#Calculate the accuracy of data generated by inverse classifier\n",
        "#Input random generated probablity input to inverse classifier to get data sample\n",
        "#Input those data samples to the classifier\n",
        "X1_Rand=InvClsfI1.predict(y1_Rand)\n",
        "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['categorical_accuracy'])\n",
        "loss, accuracy = Classfier2.evaluate(X1_Rand, y1_train)\n",
        "#print('Test score:', score)\n",
        "print('Loss:', loss)\n",
        "print('Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62079/62079 [==============================] - 3s 42us/step\n",
            "Loss: 6.5013152592982415\n",
            "Accuracy: 0.35185810923576355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtDjwey8thXI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1ec7c0f7-24ef-45f3-91bb-06b8cb02a5ae"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "Y_train = np.argmax(y1_train, axis=1)\n",
        "y_pred=Classfier2.predict_classes(X1_Rand)\n",
        "#print(keras.metrics.categorical_accuracy(y1_train, y_pred))\n",
        "allResult=classification_report(Y_train, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j19t_Wl1Sfz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92f35f03-2210-4fac-b165-171e51441a47"
      },
      "source": [
        "import ast\n",
        "print(allResult)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.85      0.78       660\n",
            "           1       0.33      0.82      0.47       625\n",
            "           2       0.00      0.00      0.00       605\n",
            "           3       1.00      0.08      0.14       635\n",
            "           4       0.05      0.01      0.02       652\n",
            "           5       0.00      0.00      0.00       618\n",
            "           6       0.00      0.00      0.00       639\n",
            "           7       0.32      0.96      0.48       617\n",
            "           8       0.95      0.88      0.92       659\n",
            "           9       0.74      0.75      0.74       635\n",
            "          10       0.01      0.05      0.02       644\n",
            "          11       1.00      0.70      0.83       628\n",
            "          12       0.99      0.94      0.96       666\n",
            "          13       0.10      0.03      0.05       641\n",
            "          14       0.00      0.00      0.00       640\n",
            "          15       0.00      0.00      0.00       658\n",
            "          16       0.00      0.00      0.00       631\n",
            "          17       0.77      0.29      0.42       651\n",
            "          18       0.47      0.64      0.54       638\n",
            "          19       0.00      0.00      0.00       642\n",
            "          20       0.36      0.75      0.48       617\n",
            "          21       0.00      0.00      0.00       642\n",
            "          22       0.69      0.88      0.77       649\n",
            "          23       0.26      0.79      0.40       646\n",
            "          24       0.13      0.13      0.13       659\n",
            "          25       0.43      0.54      0.48       649\n",
            "          26       0.68      0.62      0.65       628\n",
            "          27       0.40      0.03      0.05       613\n",
            "          28       0.02      0.01      0.01       597\n",
            "          29       0.22      0.93      0.36       645\n",
            "          30       0.82      0.41      0.54       629\n",
            "          31       0.03      0.16      0.05       652\n",
            "          32       0.53      0.55      0.54       653\n",
            "          33       0.96      0.43      0.59       615\n",
            "          34       0.43      0.84      0.57       648\n",
            "          35       0.32      0.02      0.05       644\n",
            "          36       0.97      0.79      0.87       611\n",
            "          37       0.37      0.66      0.47       642\n",
            "          38       0.00      0.00      0.00       632\n",
            "          39       0.03      0.02      0.03       638\n",
            "          40       0.44      0.78      0.57       641\n",
            "          41       0.02      0.04      0.03       636\n",
            "          42       0.42      0.74      0.54       621\n",
            "          43       0.87      0.04      0.08       622\n",
            "          44       1.00      0.02      0.04       630\n",
            "          45       0.93      0.53      0.67       647\n",
            "          46       0.96      0.28      0.43       645\n",
            "          47       0.17      0.02      0.03       648\n",
            "          48       0.00      0.00      0.00       635\n",
            "          49       0.80      0.78      0.79       617\n",
            "          50       0.28      0.70      0.40       668\n",
            "          51       0.07      0.00      0.00       633\n",
            "          52       0.19      0.01      0.02       636\n",
            "          53       0.10      0.00      0.00       650\n",
            "          54       0.00      0.00      0.00       631\n",
            "          55       0.85      0.17      0.28       644\n",
            "          56       1.00      0.06      0.11       632\n",
            "          57       0.97      0.91      0.94       666\n",
            "          58       0.27      0.61      0.37       645\n",
            "          59       0.38      0.71      0.49       645\n",
            "          60       0.06      0.08      0.07       649\n",
            "          61       0.00      0.00      0.00       656\n",
            "          62       0.78      0.05      0.09       626\n",
            "          63       0.27      0.82      0.40       619\n",
            "          64       0.00      0.00      0.00       652\n",
            "          65       0.99      0.52      0.68       607\n",
            "          66       0.33      0.43      0.37       669\n",
            "          67       0.67      0.76      0.71       661\n",
            "          68       0.00      0.00      0.00       653\n",
            "          69       0.46      0.79      0.59       664\n",
            "          70       0.15      0.26      0.19       649\n",
            "          71       0.00      0.00      0.00       618\n",
            "          72       0.27      0.89      0.41       628\n",
            "          73       0.46      0.18      0.26       632\n",
            "          74       0.00      0.00      0.00       651\n",
            "          75       0.00      0.00      0.00       624\n",
            "          76       0.00      0.00      0.00       629\n",
            "          77       0.07      0.09      0.08       643\n",
            "          78       0.98      0.64      0.78       650\n",
            "          79       0.49      0.91      0.63       642\n",
            "          80       0.18      0.85      0.30       652\n",
            "          81       0.00      0.00      0.00       674\n",
            "          82       0.00      0.00      0.00       651\n",
            "          83       0.68      0.09      0.16       643\n",
            "          84       0.17      0.01      0.02       636\n",
            "          85       0.44      0.88      0.59       651\n",
            "          86       0.16      0.09      0.11       640\n",
            "          87       0.03      0.04      0.04       661\n",
            "          88       0.06      0.01      0.02       636\n",
            "          89       0.00      0.00      0.00       636\n",
            "          90       0.46      0.38      0.42       642\n",
            "          91       0.00      0.00      0.00       625\n",
            "          92       0.69      0.66      0.68       627\n",
            "          93       0.75      0.83      0.79       659\n",
            "          94       0.94      0.89      0.91       660\n",
            "          95       0.00      0.00      0.00       632\n",
            "          96       0.99      0.87      0.93       647\n",
            "\n",
            "    accuracy                           0.35     62079\n",
            "   macro avg       0.38      0.35      0.30     62079\n",
            "weighted avg       0.38      0.35      0.30     62079\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKkZ47RyttEK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a84c9abe-208c-4776-8523-7070541d5e70"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "#ax = fig.add_axes([0,0,97,1])\n",
        "listu=np.arange(0, 97, 1).tolist()\n",
        "accept = [0.85,0.82,0.00,0.08,0.01,0.00,0.00,0.96,0.88,0.75,0.05,0.70,0.94,0.03,0.00,0.00,0.00,0.29,0.64,0.00,\n",
        "          0.75,0.00,0.88,0.79,0.13,0.54,0.62,0.03,0.01,0.93,0.41,0.16,0.55,0.43,0.84,0.02,0.79,0.66,0.00,0.02,\n",
        "          0.78,0.04,0.74,0.04,0.02,0.53,0.28,0.02,0.00,0.78,0.70,0.00,0.01,0.00,0.00,0.17,0.06,0.91,0.61,0.71,\n",
        "          0.08,0.00,0.05,0.82,0.00,0.52,0.43,0.76,0.00,0.79,0.26,0.00,0.89,0.18,0.00,0.00,0.00,0.09,0.64,0.91,\n",
        "          0.85,0.00,0.00,0.09,0.01,0.88,0.09,0.04,0.01,0.00,0.38,0.00,0.66,0.83,0.89,0.00,0.87]\n",
        "accept=np.array(accept) \n",
        "accept=100 *accept         \n",
        "plt.bar(listu,accept)\n",
        "plt.ylabel('percentage')\n",
        "plt.xlabel('user index')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}